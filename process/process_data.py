import os
import re
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings

# --- C·∫§U H√åNH ---
DATA_PATH = "crawler/output_texts"
DB_PATH = "vector_store/faiss"
EMBEDDING_MODEL_PATH = "model/all-MiniLM-L6-v2-f16.gguf"
MAX_CHUNK_SIZE = 1200  # K√≠ch th∆∞·ªõc t·ªëi ƒëa cho m·ªôt chunk (t√≠nh b·∫±ng k√Ω t·ª±)
CHUNK_OVERLAP = 300    # ƒê·ªô ch·ªìng l·∫•n gi·ªØa c√°c chunk b·ªã c·∫Øt

def preprocess_text(text: str) -> str:
    """
    H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n:
    - N·ªëi c√°c d√≤ng b·ªã ng·∫Øt kh√¥ng t·ª± nhi√™n.
    - Chu·∫©n h√≥a kho·∫£ng tr·∫Øng.
    - Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng th·ª´a.
    """
    
    text = re.sub(r'(?<!\n)\n(?!\n)', ' ', text)
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    return text.strip()

def build_vector_store():
    
    print(f"ƒêang t·∫£i c√°c file t·ª´ th∆∞ m·ª•c: '{DATA_PATH}'...")
    if not os.path.exists(DATA_PATH):
        print(f"L·ªói: Th∆∞ m·ª•c '{DATA_PATH}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")
        return
        
    loader = DirectoryLoader(
        DATA_PATH, glob="*.txt", loader_cls=TextLoader,
        loader_kwargs={'encoding': 'utf-8'}, show_progress=True,
    )
    documents = loader.load()
    if not documents:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file .txt n√†o trong '{DATA_PATH}'.")
        return
    print(f"ƒê√£ t·∫£i th√†nh c√¥ng {len(documents)} t√†i li·ªáu.")

    print("\nB·∫Øt ƒë·∫ßu qu√° tr√¨nh ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n...")
    for doc in documents:
        doc.page_content = preprocess_text(doc.page_content)
    print("Ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t.")

    print("\nB·∫Øt ƒë·∫ßu qu√° tr√¨nh chia chunk theo k√Ω t·ª±...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=MAX_CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", "; ", ", ", " "], 
        length_function=len
    )
    all_chunks = text_splitter.split_documents(documents)
    
    if not all_chunks:
        print("L·ªói: Kh√¥ng t·∫°o ƒë∆∞·ª£c chunk n√†o t·ª´ c√°c t√†i li·ªáu.")
        return
        
    print(f"Ho√†n t·∫•t! ƒê√£ t·∫°o t·ªïng c·ªông {len(all_chunks)} chunks.")
    
    print("\nƒêang kh·ªüi t·∫°o m√¥ h√¨nh embeddings...")

    model_path= "models/vietnamese-bi-encoder"
    model_kwargs = {'device': 'cpu'} 
    encode_kwargs = {'normalize_embeddings': True}
    
    # embeddings = GPT4AllEmbeddings(
    #     model_file=EMBEDDING_MODEL_PATH,
    # )

    embeddings = HuggingFaceEmbeddings(
        model_name=model_path,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
)
    
    print("ƒêang t·∫°o vector store t·ª´ c√°c chunks (qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")
    vector_store = FAISS.from_documents(
        documents=all_chunks,
        embedding=embeddings
    )
    
    if not os.path.exists("vector_store"):
        os.makedirs("vector_store")
    vector_store.save_local(DB_PATH)
    print(f"\nüéâ Vector store ƒë√£ ƒë∆∞·ª£c t·∫°o v√† l∆∞u th√†nh c√¥ng t·∫°i: '{DB_PATH}'")

if __name__ == "__main__":
    build_vector_store()