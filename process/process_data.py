import os
import re
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

DATA_PATH = "crawler\output_texts"
DB_PATH = "vector_store/faiss"
EMBEDDING_MODEL_PATH = "model/all-MiniLM-L6-v2-f16.gguf"


def refined_clean_legal_text(raw_text: str) -> str:
    
    lines = raw_text.split('\n')
    processed_lines = []
    temp_line = ""
    for line in lines:
        stripped_line = line.strip()
        if not stripped_line:
            continue
        
        temp_line = (temp_line + " " + stripped_line).strip()
        
        # Coi m·ªôt d√≤ng l√† ho√†n ch·ªânh n·∫øu n√≥ k·∫øt th√∫c b·∫±ng d·∫•u c√¢u m·∫°nh ho·∫∑c l√† ti√™u ƒë·ªÅ
        if temp_line.endswith(('.', ':', ';', '‚Äù', '/.')) or temp_line.isupper():
            processed_lines.append(temp_line)
            temp_line = ""
    if temp_line:
        processed_lines.append(temp_line)

    text = "\n".join(processed_lines)

    patterns_to_remove = [
        # X√≥a Ti√™u ng·ªØ v√† Qu·ªëc hi·ªáu
        r'C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM\s*ƒê·ªôc l·∫≠p - T·ª± do - H·∫°nh ph√∫c',
        # X√≥a c√°c d√≤ng g·∫°ch ngang trang tr√≠
        r'^\s*-{5,}\s*$',
        # X√≥a kh·ªëi Ch·ªØ k√Ω ·ªü cu·ªëi vƒÉn b·∫£n
        r'TM\.\s*·ª¶Y BAN NH√ÇN D√ÇN\s*CH·ª¶ T·ªäCH\s*[\w\s]+',
        # X√≥a kh·ªëi "N∆°i nh·∫≠n" ·ªü cu·ªëi vƒÉn b·∫£n
        r'N∆°i nh·∫≠n:[\s\S]*?(?=\Z)', # \Z kh·ªõp v·ªõi cu·ªëi c√πng c·ªßa chu·ªói
    ]
    for pattern in patterns_to_remove:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)
        
    # 3. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng v√† c√°c d√≤ng tr·ªëng th·ª´a
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)

    return text.strip()


def build_vector_store():
  
    print(f"ƒêang t·∫£i c√°c file t·ª´ th∆∞ m·ª•c: '{DATA_PATH}'...")
    loader = DirectoryLoader(
        DATA_PATH,
        glob="*.txt",
        loader_cls=TextLoader,
        loader_kwargs={'encoding': 'utf-8'}
    )
    documents = loader.load()
    if not documents:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file .txt n√†o trong th∆∞ m·ª•c '{DATA_PATH}'. Vui l√≤ng ki·ªÉm tra l·∫°i.")
        return
    print(f"ƒê√£ t·∫£i {len(documents)} t√†i li·ªáu.")

    print("ƒêang l√†m s·∫°ch n·ªôi dung c√°c t√†i li·ªáu...")
    for doc in documents:
        
        doc.page_content = refined_clean_legal_text(doc.page_content)
        

    print("L√†m s·∫°ch ho√†n t·∫•t.")

    # 3. Chia nh·ªè vƒÉn b·∫£n (Chunking)
    print("ƒêang chia nh·ªè t√†i li·ªáu th√†nh c√°c chunks...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", ".", " "]
    )
    chunks = splitter.split_documents(documents)
    print(f"ƒê√£ t·∫°o {len(chunks)} chunks t·ª´ c√°c t√†i li·ªáu.")

    # 4. T·∫°o embedding v√† l∆∞u v√†o FAISS
    print("ƒêang kh·ªüi t·∫°o m√¥ h√¨nh embeddings...")
    embeddings_model = GPT4AllEmbeddings(model_file=EMBEDDING_MODEL_PATH)
    
    print("ƒêang t·∫°o vector store t·ª´ c√°c chunks (qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")
    vector_store = FAISS.from_documents(
        chunks,
        embeddings_model
    )
    
    # 5. L∆∞u vector store ra ƒëƒ©a
    if not os.path.exists("vector_store"):
        os.makedirs("vector_store")
    vector_store.save_local(DB_PATH)
    print(f"üéâ Vector store ƒë√£ ƒë∆∞·ª£c t·∫°o v√† l∆∞u th√†nh c√¥ng t·∫°i: '{DB_PATH}'")


if __name__ == "__main__":
    build_vector_store()