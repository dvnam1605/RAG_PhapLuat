from query_transform import transformed_search
import os
import google.generativeai as genai
from dotenv import load_dotenv
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS




API_KEY = os.getenv("API_KEY")
if not API_KEY:
    raise ValueError("API key for Google Generative AI is not set in the environment variables.")
MODEL = "gemini-1.5-flash"
embeddings = GPT4AllEmbeddings(model_file="model/all-MiniLM-L6-v2-f16.gguf")


genai.configure(api_key=API_KEY)
model = genai.GenerativeModel(MODEL)

path_db = "vector_store/faiss"
# Táº£i láº¡i vector store tá»« file Ä‘Ã£ lÆ°u
vector_store = FAISS.load_local(
    path_db, 
    embeddings,
    allow_dangerous_deserialization=True  # Add this parameter
)

number_retrievals = 10 # Sá»‘ lÆ°á»£ng vÄƒn báº£n cuá»‘i cÃ¹ng tráº£ vá»
mmr_retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={
        'k': number_retrievals, # Sá»‘ lÆ°á»£ng vÄƒn báº£n cuá»‘i cÃ¹ng tráº£ vá»
        'fetch_k': 100, # Sá»‘ lÆ°á»£ng vÄƒn báº£n ban Ä‘áº§u cáº§n láº¥y Ä‘á»ƒ chá»n lá»c, nÃªn lá»›n hÆ¡n k
        'lambda_mult': 0.3 # GiÃ¡ trá»‹ tá»« 0 Ä‘áº¿n 1. 0.5 lÃ  cÃ¢n báº±ng, 1 lÃ  diversity, 0 lÃ  similarity.
    }
)


def generate_response(query, context):
    """
    Generates a response based on the query and context using Gemini.
    
    Args:
        query (str): The user query
        context (str): Contextual information to include in the response
        
    Returns:
        str: The generated response
    """
    system_prompt = (
    "Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn nghiá»‡p trong viá»‡c tráº£ lá»i cÃ¢u há»i thuá»™c lÄ©nh vá»±c phÃ¡p luáº­t, Ä‘Æ°á»£c tÃ­ch há»£p trong há»‡ thá»‘ng RAG dá»±a trÃªn truy xuáº¥t vÄƒn báº£n phÃ¡p lÃ½. "
    "Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  sá»­ dá»¥ng **duy nháº¥t cÃ¡c thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p trong pháº§n 'Bá»‘i cáº£nh'** Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i phÃ¡p lÃ½ do ngÆ°á»i dÃ¹ng Ä‘áº·t ra. "
    "KhÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i, khÃ´ng Ä‘Æ°á»£c suy luáº­n vÆ°á»£t quÃ¡ pháº¡m vi thÃ´ng tin cÃ³ sáºµn, vÃ  khÃ´ng Ä‘Æ°á»£c giáº£ Ä‘á»‹nh hoáº·c phá»ng Ä‘oÃ¡n. "
    "Náº¿u pháº§n Bá»‘i cáº£nh khÃ´ng chá»©a Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ tráº£ lá»i, báº¡n **pháº£i** tráº£ lá»i Ä‘Ãºng má»™t cÃ¢u: **'KhÃ´ng cÃ³ thÃ´ng tin.'** "
    "CÃ¢u tráº£ lá»i cáº§n ngáº¯n gá»n, rÃµ rÃ ng, chuáº©n xÃ¡c, cÃ³ thá»ƒ trÃ­ch dáº«n láº¡i ngáº¯n gá»n quy Ä‘á»‹nh phÃ¡p lÃ½ náº¿u cáº§n thiáº¿t, vÃ  nÃªn thá»ƒ hiá»‡n ngÃ´n ngá»¯ trung láº­p, khÃ¡ch quan nhÆ° trong cÃ¡c tÃ i liá»‡u phÃ¡p luáº­t."
)

    
    user_prompt = f"""
        Báº¡n sáº½ Ä‘Æ°á»£c cung cáº¥p má»™t cÃ¢u há»i phÃ¡p lÃ½ tá»« ngÆ°á»i dÃ¹ng vÃ  pháº§n Bá»‘i cáº£nh chá»©a thÃ´ng tin Ä‘Æ°á»£c truy xuáº¥t tá»« cÃ¡c vÄƒn báº£n luáº­t, quy Ä‘á»‹nh, nghá»‹ Ä‘á»‹nh, hoáº·c Ã¡n lá»‡.

        ğŸ”’ **YÃªu cáº§u báº¯t buá»™c khi tráº£ lá»i:**
        - Chá»‰ sá»­ dá»¥ng thÃ´ng tin trong pháº§n Bá»‘i cáº£nh Ä‘á»ƒ tráº£ lá»i. KhÃ´ng Ä‘Æ°á»£c suy luáº­n thÃªm, khÃ´ng thÃªm vÃ­ dá»¥ náº¿u khÃ´ng cÃ³ trong bá»‘i cáº£nh.
        - Náº¿u bá»‘i cáº£nh **khÃ´ng chá»©a thÃ´ng tin phÃ¹ há»£p** hoáº·c khÃ´ng Ä‘á»§ Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c, hÃ£y tráº£ lá»i Ä‘Ãºng má»™t cÃ¢u: **"KhÃ´ng cÃ³ thÃ´ng tin."**
        - Náº¿u cÃ³ thá»ƒ, hÃ£y giá»¯ láº¡i ngÃ´n ngá»¯ phÃ¡p lÃ½ trung láº­p (nhÆ° â€œtheo quy Ä‘á»‹nhâ€, â€œngÆ°á»i sá»­ dá»¥ng lao Ä‘á»™ng cÃ³ nghÄ©a vá»¥â€, â€œcÆ¡ quan cÃ³ tháº©m quyá»nâ€¦â€).
        - KhÃ´ng cáº§n chÃ o há»i hoáº·c giáº£i thÃ­ch dÃ i dÃ²ng. Tráº£ lá»i trá»±c tiáº¿p, sÃºc tÃ­ch vÃ  chÃ­nh xÃ¡c.

        ---

        ğŸ“Œ **CÃ¢u há»i:**  
        {query}

        ğŸ“š **Bá»‘i cáº£nh (Ä‘Æ°á»£c truy xuáº¥t):**  
        {context}

        ---

        ğŸ§  **Tráº£ lá»i:**
        """

    full_prompt = f"{system_prompt}\n\n{user_prompt}"
    response = model.generate_content(
        full_prompt,
        generation_config=genai.types.GenerationConfig(
            temperature=0.0  # Giá»¯ Ä‘á»™ ngáº«u nhiÃªn tháº¥p Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c
        )
    )
    return response.text.strip()

def rag_with_query_transformation(query, transformation_type=None):
    """
    Perform RAG with query transformation.
    
    Args:
        pdf_path (str): Path to the PDF file
        query (str): User query
        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')
        
    Returns:
        str: Generated response based on the query and context
    """
        
    if transformation_type:
        # Perform transformed search
        results = transformed_search(query, transformation_type)
    else:
        # Perform regular search
        results = mmr_retriever.invoke(query)

    context = "\n\n".join([f"PASSAGE {i+1}:\n{result.page_content}" for i, result in enumerate(results)])
    response = generate_response(query, context)

    return {
        "CÃ¢u há»i gá»‘c": query,
        "Dáº¡ng biáº¿n Ä‘á»•i": transformation_type,
        "Ngá»¯ cáº£nh": context,
        "Tráº£ lá»i": response
    }

if __name__ == "__main__":

    query = input("Nháº­p cÃ¢u há»i cá»§a báº¡n: ")
    transformation_type = input("Nháº­p dáº¡ng biáº¿n Ä‘á»•i (rewrite, step_back, decompose) hoáº·c Ä‘á»ƒ trá»‘ng náº¿u khÃ´ng cáº§n: ").strip() or None
    result = rag_with_query_transformation(query, transformation_type)
    print(f"Káº¿t quáº£ tráº£ vá»: {result["Tráº£ lá»i"]}")
   