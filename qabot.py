import os
import re
import time
from typing import TypedDict, List, Literal, Optional

import google.generativeai as genai
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS
from duckduckgo_search import DDGS
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever

load_dotenv()
API_KEY = os.getenv("API_KEY")
if not API_KEY:
    raise ValueError("API key for Google Generative AI is not set in the environment variables.")

# --- C√ÅC H·∫∞NG S·ªê ---
MODEL_NAME = "gemini-1.5-flash"
EMBEDDING_MODEL_PATH = "model/all-MiniLM-L6-v2-f16.gguf"
VECTOR_STORE_PATH = "vector_store/faiss"
MAX_WEB_RESULTS = 3 # Gi·∫£m s·ªë k·∫øt qu·∫£ ƒë·ªÉ tr√°nh RateLimit

print("ƒêang c·∫•u h√¨nh c√°c m√¥ h√¨nh...")
genai.configure(api_key=API_KEY)
llm_model = genai.GenerativeModel(MODEL_NAME)
embeddings = GPT4AllEmbeddings(model_file=EMBEDDING_MODEL_PATH)

try:
    print(f"ƒêang t·∫£i Vector Store t·ª´ '{VECTOR_STORE_PATH}'...")
    vector_store = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)
    retriever = vector_store.as_retriever(
                                            search_type="similarity",
                                            search_kwargs={'k': 5}
                                        )
    print("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng.\n")
except Exception as e:
    print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ t·∫£i Vector Store. Vui l√≤ng ch·∫°y ch·ª©c nƒÉng 'build' tr∆∞·ªõc.")
    print(f"   L·ªói chi ti·∫øt: {e}")
    retriever = None # ƒê·∫∑t retriever l√† None n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c


# ==============================================================================
# PH·∫¶N 1: LOGIC BI·∫æN ƒê·ªîI TRUY V·∫§N (QUERY TRANSFORMATION)
# ==============================================================================

def rewrite_to_general_query(model: genai.GenerativeModel, query: str) -> str:
    """Vi·∫øt l·∫°i c√¢u h·ªèi g·ªëc th√†nh M·ªòT c√¢u h·ªèi kh√°c h·ª£p l√Ω v√† kh√°i qu√°t h∆°n."""
    prompt = f"""B·∫°n l√† m·ªôt Tr·ª£ l√Ω AI ph√°p l√Ω chuy√™n nghi·ªáp. Nhi·ªám v·ª• c·ªßa b·∫°n l√† nh·∫≠n m·ªôt c√¢u h·ªèi ph√°p l√Ω t·ª´ ng∆∞·ªùi d√πng v√† **vi·∫øt l·∫°i n√≥ th√†nh M·ªòT c√¢u h·ªèi ph√°p l√Ω kh√°c, h·ª£p l√Ω v√† kh√°i qu√°t h∆°n**.
M·ª•c ti√™u l√† t√¨m ra c√°c vƒÉn b·∫£n lu·∫≠t, ngh·ªã ƒë·ªãnh, th√¥ng t∆∞ n·ªÅn t·∫£ng li√™n quan ƒë·∫øn v·∫•n ƒë·ªÅ g·ªëc.

**QUY T·∫ÆC:**
1.  **KH√îNG TR·∫¢ L·ªúI C√ÇU H·ªéI.** Ch·ªâ t·∫≠p trung v√†o vi·ªác t·∫°o ra c√¢u h·ªèi m·ªõi.
2.  **KH√ÅI QU√ÅT H√ìA:** Chuy·ªÉn c√¢u h·ªèi chi ti·∫øt th√†nh c√¢u h·ªèi v·ªÅ nguy√™n t·∫Øc, quy ƒë·ªãnh chung.
3.  **CH·ªà M·ªòT C√ÇU H·ªéI.**

---
**V√ç D·ª§:**
**C√¢u h·ªèi g·ªëc:** "C√¥ng ty n·ª£ l∆∞∆°ng 2 th√°ng th√¨ ph·∫°t th·∫ø n√†o?"
**C√¢u h·ªèi kh√°i qu√°t h∆°n:** "Quy ƒë·ªãnh v·ªÅ tr√°ch nhi·ªám ph√°p l√Ω c·ªßa ng∆∞·ªùi s·ª≠ d·ª•ng lao ƒë·ªông ƒë·ªëi v·ªõi vi·ªác ch·∫≠m tr·∫£ l∆∞∆°ng cho ng∆∞·ªùi lao ƒë·ªông l√† g√¨?"
---
**Y√äU C·∫¶U:** Vi·∫øt l·∫°i c√¢u h·ªèi d∆∞·ªõi ƒë√¢y th√†nh M·ªòT c√¢u h·ªèi kh√°i qu√°t h∆°n.
**C√¢u h·ªèi g·ªëc:** "{query}"
**C√¢u h·ªèi kh√°i qu√°t h∆°n:**
"""
    response = model.generate_content(prompt, generation_config=genai.types.GenerationConfig(temperature=0.2))
    return response.text.strip()

def decompose_query(model: genai.GenerativeModel, query: str) -> List[str]:
    """Ph√¢n r√£ c√¢u h·ªèi ph·ª©c t·∫°p th√†nh c√°c c√¢u h·ªèi con."""
    prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch ph√°p l√Ω. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n r√£ m·ªôt c√¢u h·ªèi ph√°p l√Ω **ph·ª©c t·∫°p** th√†nh nhi·ªÅu c√¢u h·ªèi con, **ƒë∆°n gi·∫£n v√† ƒë·ªôc l·∫≠p**.
**QUY T·∫ÆC:** M·ªói c√¢u h·ªèi con ph·∫£i t·∫≠p trung v√†o **M·ªòT** kh√≠a c·∫°nh duy nh·∫•t. M·ªói c√¢u h·ªèi con tr√™n m·ªôt d√≤ng.
---
**V√ç D·ª§:**
**C√¢u h·ªèi g·ªëc:** "T√¥i mu·ªën ly h√¥n ƒë∆°n ph∆∞∆°ng khi ch·ªìng t√¥i c√≥ h√†nh vi b·∫°o l·ª±c gia ƒë√¨nh v√† ƒëang tr·ªën n·ª£, th·ªß t·ª•c c·∫ßn nh·ªØng g√¨ v√† t√†i s·∫£n chung l√† m·ªôt ng√¥i nh√† s·∫Ω ƒë∆∞·ª£c ph√¢n chia ra sao?"
**C√¢u h·ªèi con ƒë∆∞·ª£c ph√¢n r√£:**
CƒÉn c·ª© ph√°p l√Ω ƒë·ªÉ ly h√¥n ƒë∆°n ph∆∞∆°ng khi c√≥ h√†nh vi b·∫°o l·ª±c gia ƒë√¨nh l√† g√¨?
Th·ªß t·ª•c v√† h·ªì s∆° c·∫ßn thi·∫øt ƒë·ªÉ ti·∫øn h√†nh ly h√¥n ƒë∆°n ph∆∞∆°ng t·∫°i T√≤a √°n?
Nguy√™n t·∫Øc ph√¢n chia t√†i s·∫£n chung l√† nh√† ·ªü khi ly h√¥n ƒë∆∞·ª£c quy ƒë·ªãnh nh∆∞ th·∫ø n√†o?
---
**Y√äU C·∫¶U:** Ph√¢n r√£ c√¢u h·ªèi ph·ª©c t·∫°p d∆∞·ªõi ƒë√¢y.
**C√¢u h·ªèi g·ªëc:** "{query}"
**C√¢u h·ªèi con ƒë∆∞·ª£c ph√¢n r√£:**
"""
    response = model.generate_content(prompt, generation_config=genai.types.GenerationConfig(temperature=0.2))
    sub_queries = response.text.strip().split('\n')
    cleaned_queries = [re.sub(r'^\s*-\s*|\s*\d+\.\s*', '', q).strip() for q in sub_queries if q.strip()]
    return cleaned_queries if cleaned_queries else [query]

def _search_multiple_queries(queries: List[str], retriever: BaseRetriever, k_per_query: int = 3) -> List[Document]:
    """H√†m helper ƒë·ªÉ t√¨m ki·∫øm cho nhi·ªÅu truy v·∫•n v√† g·ªôp k·∫øt qu·∫£."""
    all_results = []
    print("---ƒêang th·ª±c hi·ªán t√¨m ki·∫øm cho c√°c truy v·∫•n con---")
    for sub_q in queries:
        print(f"  -> ƒêang t√¨m ki·∫øm cho: '{sub_q}'")
        try:
            all_results.extend(retriever.invoke(sub_q, k=k_per_query))
        except Exception as e:
            print(f"  L·ªói khi t√¨m ki·∫øm cho '{sub_q}': {e}")
            continue
    unique_docs = list({doc.page_content: doc for doc in all_results}.values())
    print(f"---T√¨m th·∫•y t·ªïng c·ªông {len(all_results)} t√†i li·ªáu, sau khi l·ªçc c√≤n {len(unique_docs)} t√†i li·ªáu duy nh·∫•t.---")
    return unique_docs

def transformed_search(query: str, transformation_type: str, model: genai.GenerativeModel, retriever: BaseRetriever) -> List[Document]:
    """Th·ª±c hi·ªán t√¨m ki·∫øm s·ª≠ d·ª•ng truy v·∫•n ƒë√£ ƒë∆∞·ª£c bi·∫øn ƒë·ªïi."""
    if transformation_type == "rewrite":
        print("üîé B·∫Øt ƒë·∫ßu bi·∫øn ƒë·ªïi truy v·∫•n: REWRITE (Th√†nh 1 c√¢u kh√°i qu√°t)")
        transformed_query = rewrite_to_general_query(model, query)
        print(f"  -> C√¢u h·ªèi kh√°i qu√°t h∆°n: {transformed_query}")
        return retriever.invoke(transformed_query)
    elif transformation_type == "step_back" or transformation_type == "decompose": # G·ªôp step_back v√† decompose
        print(f"üîé B·∫Øt ƒë·∫ßu bi·∫øn ƒë·ªïi truy v·∫•n: {transformation_type.upper()}")
        queries = decompose_query(model, query) # C·∫£ hai ƒë·ªÅu c√≥ th·ªÉ d√πng logic ph√¢n r√£
        return _search_multiple_queries(queries, retriever)
    print("üîé Th·ª±c hi·ªán t√¨m ki·∫øm th√¥ng th∆∞·ªùng (kh√¥ng bi·∫øn ƒë·ªïi)")
    return retriever.invoke(query)


# ==============================================================================
# PH·∫¶N 2: LOGIC ƒê·ªí TH·ªä LANGGRAPH (RAG PIPELINE)
# ==============================================================================

class GraphState(TypedDict):
    query: str
    result: str
    documents: List[str]
    prompt_template: str
    history: List[tuple]
    transformation_type: Optional[str]
    route_decision: str
    web_search_failed: bool = False

# --- C√ÅC NODE C·ª¶A ƒê·ªí TH·ªä ---

def handle_internal_search(state: GraphState) -> dict:
    if state.get('web_search_failed', False):
        print("---NODE: T√¨m ki·∫øm web th·∫•t b·∫°i, chuy·ªÉn sang t√¨m ki·∫øm n·ªôi b·ªô (FAISS)---")
    else:
        print("---NODE: Th·ª±c hi·ªán t√¨m ki·∫øm n·ªôi b·ªô (FAISS)---")

    query = state['query']
    transformation_type = state.get('transformation_type', None)
    print(f"---S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p bi·∫øn ƒë·ªïi: {transformation_type.upper() if transformation_type else 'REGULAR'}---")
    results = transformed_search(query=query, transformation_type=transformation_type, model=llm_model, retriever=retriever)
    
    context = "\n\n---\n\n".join([doc.page_content for doc in results]) if results else "Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c t√¨m th·∫•y."
    prompt_template = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω ph√°p l√Ω AI th√¥ng minh v√† trung th·ª±c. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng CH·ªà d·ª±a v√†o ph·∫ßn "D·ªØ li·ªáu n·ªôi b·ªô" ƒë∆∞·ª£c cung c·∫•p. Lu√¥n trung th·ª±c, n·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i r√µ l√† kh√¥ng c√≥.
**D·ªØ li·ªáu n·ªôi b·ªô:**
---
{context}
---
**C√¢u h·ªèi:** {query}
**Tr·∫£ l·ªùi:**
"""
    documents = [doc.page_content for doc in results]
    return {"documents": documents, "prompt_template": prompt_template, "web_search_failed": False}

def handle_web_search(state: GraphState) -> dict:
    print("---NODE: Th·ª±c hi·ªán t√¨m ki·∫øm tr√™n web (DuckDuckGo)---")
    query = state['query']
    prompt_template = f"""D·ª±a CH·ª¶ Y·∫æU v√†o c√°c k·∫øt qu·∫£ t√¨m ki·∫øm sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c. Tr√≠ch d·∫´n ngu·ªìn n·∫øu c√≥ th·ªÉ.
**K·∫øt qu·∫£ t√¨m ki·∫øm:**
---
{{context}}
---
**C√¢u h·ªèi:** {query}
**Tr·∫£ l·ªùi:**
"""
    max_retries = 2
    initial_delay = 1
    for attempt in range(max_retries):
        try:
            print(f"ƒêang g·ª≠i y√™u c·∫ßu ƒë·∫øn DuckDuckGo... (L·∫ßn th·ª≠ {attempt + 1}/{max_retries})")
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=MAX_WEB_RESULTS))
            if results:
                documents = [f"Ngu·ªìn: {r.get('href')}\nN·ªôi dung: {r.get('body')}" for r in results if r.get('body')]
                if documents:
                     return {"documents": documents, "prompt_template": prompt_template, "web_search_failed": False}
            print("--- T√¨m ki·∫øm web kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£ n√†o. Coi nh∆∞ th·∫•t b·∫°i. ---")
            return {"web_search_failed": True, "documents": []}
        except Exception as e:
            print(f"L·ªói khi t√¨m ki·∫øm tr√™n web (l·∫ßn th·ª≠ {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(initial_delay * (2 ** attempt))
            else:
                return {"web_search_failed": True, "documents": []}
    return {"web_search_failed": True, "documents": []}

def generate_final_answer(state: GraphState) -> dict:
    print("---NODE: Sinh c√¢u tr·∫£ l·ªùi cu·ªëi c√πng---")
    if not state.get('documents'):
        result = "R·∫•t ti·∫øc, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c·∫£ c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô v√† tr√™n web ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n."
        return {"result": result}
        
    context = "\n\n---\n\n".join(state['documents'])
    final_prompt = state['prompt_template'].format(context=context, query=state['query'])
    response = llm_model.generate_content(final_prompt)
    return {"result": response.text.strip()}

def final_router(state: GraphState) -> dict:
    print("---ROUTER (FINAL): Th·ª±c thi ƒë·ªãnh tuy·∫øn d·ª±a tr√™n m·ªánh l·ªánh---")
    query = state['query']
    try:
        relevant_docs = retriever.invoke(query, k=1)
        evidence = relevant_docs[0].page_content if relevant_docs else "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o trong c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô."
    except Exception as e:
        evidence = f"L·ªói khi truy v·∫•n c∆° s·ªü d·ªØ li·ªáu n·ªôi b·ªô: {e}"

    routing_prompt = f"""**M·ªÜNH L·ªÜNH H·ªÜ TH·ªêNG:** B·∫†N L√Ä M·ªòT B·ªò ƒê·ªäNH TUY·∫æN. NHI·ªÜM V·ª§ C·ª¶A B·∫†N L√Ä ∆ØU TI√äN S·ª¨ D·ª§NG C∆† S·ªû D·ªÆ LI·ªÜU N·ªòI B·ªò.
**QUY T·∫ÆC:**
1.  **PH√ÇN T√çCH B·∫∞NG CH·ª®NG:** Xem x√©t k·ªπ ph·∫ßn `B·∫∞NG CH·ª®NG T·ª™ CSDL N·ªòI B·ªò` d∆∞·ªõi ƒë√¢y.
2.  **RA QUY·∫æT ƒê·ªäNH:**
    *   N·∫øu `B·∫∞NG CH·ª®NG` c√≥ ch·ª©a th√¥ng tin li√™n quan ƒë·∫øn `C√ÇU H·ªéI`, B·∫†N **B·∫ÆT BU·ªòC** PH·∫¢I ch·ªçn `internal_search`.
    *   B·∫°n CH·ªà ƒë∆∞·ª£c ph√©p ch·ªçn `web_search` khi `B·∫∞NG CH·ª®NG` ho√†n to√†n kh√¥ng li√™n quan.
**KHI C√ì B·∫§T K·ª≤ NGHI NG·ªú N√ÄO, LU√îN M·∫∂C ƒê·ªäNH CH·ªåN `internal_search`.**
---
**B·∫∞NG CH·ª®NG T·ª™ CSDL N·ªòI B·ªò:**
{evidence}
---
**C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG:**
"{query}"
---
**QUY·∫æT ƒê·ªäNH C·ª¶A B·∫†N (CH·ªà M·ªòT T·ª™):**
"""
    response = llm_model.generate_content(routing_prompt)
    decision_text = response.text.strip().lower()
    if "internal_search" in decision_text:
        print("---ROUTER (FINAL): Quy·∫øt ƒë·ªãnh -> T√¨m ki·∫øm n·ªôi b·ªô (internal_search)---")
        return {"route_decision": "internal_search"}
    else:
        print("---ROUTER (FINAL): Quy·∫øt ƒë·ªãnh -> T√¨m ki·∫øm tr√™n web (web_search)---")
        return {"route_decision": "web_search"}

# --- C√ÅC H√ÄM ƒêI·ªÄU KI·ªÜN C·ª¶A ƒê·ªí TH·ªä ---

def decide_initial_route(state: GraphState) -> Literal["internal_search", "web_search"]:
    """Quy·∫øt ƒë·ªãnh h∆∞·ªõng ƒëi ban ƒë·∫ßu."""
    return state["route_decision"]

def decide_after_web_search(state: GraphState) -> Literal["generate_answer", "internal_search"]:
    """Quy·∫øt ƒë·ªãnh sau khi t√¨m ki·∫øm web, th·ª±c hi·ªán logic fallback."""
    if state.get('web_search_failed', False):
        print("---CONDITION: T√¨m ki·∫øm web th·∫•t b·∫°i, chuy·ªÉn h∆∞·ªõng sang internal_search.---")
        return "internal_search"
    else:
        print("---CONDITION: T√¨m ki·∫øm web th√†nh c√¥ng, chuy·ªÉn h∆∞·ªõng sang generate_answer.---")
        return "generate_answer"

# --- X√ÇY D·ª∞NG V√Ä BI√äN D·ªäCH ƒê·ªí TH·ªä ---
print("ƒêang x√¢y d·ª±ng ƒë·ªì th·ªã LangGraph...")
workflow = StateGraph(GraphState)

workflow.add_node("router", final_router)
workflow.add_node("internal_search", handle_internal_search)
workflow.add_node("web_search", handle_web_search)
workflow.add_node("generate_answer", generate_final_answer)

workflow.set_entry_point("router")
workflow.add_conditional_edges("router", decide_initial_route, {"internal_search": "internal_search", "web_search": "web_search"})
workflow.add_conditional_edges("web_search", decide_after_web_search, {"generate_answer": "generate_answer", "internal_search": "internal_search"})
workflow.add_edge("internal_search", "generate_answer")
workflow.add_edge("generate_answer", END)

app = workflow.compile()
print("‚úÖ LangGraph v·ªõi logic fallback ƒë√£ ƒë∆∞·ª£c bi√™n d·ªãch th√†nh c√¥ng!")


# ==============================================================================
# PH·∫¶N 3: H√ÄM WRAPPER V√Ä V√íNG L·∫∂P CH√çNH
# ==============================================================================

def rag_pipeline(query: str, history: List[tuple], transformation_type: Optional[str]):
    """H√†m ch√≠nh ƒë·ªÉ g·ªçi v√† ch·∫°y LangGraph."""
    if not retriever:
        return {"C√¢u h·ªèi": query, "Tr·∫£ l·ªùi": "L·ªói: Vector store ch∆∞a ƒë∆∞·ª£c t·∫£i. Kh√¥ng th·ªÉ th·ª±c hi·ªán truy v·∫•n.", "Ph∆∞∆°ng ph√°p": "L·ªói h·ªá th·ªëng"}
    inputs = {"query": query, "history": history, "transformation_type": transformation_type}
    final_state = app.invoke(inputs)
    route_decision = final_state.get('route_decision', 'Kh√¥ng r√µ')
    final_decision_text = "Tra c·ª©u vƒÉn b·∫£n ph√°p lu·∫≠t" if route_decision == "internal_search" else "T√¨m ki·∫øm tr√™n Internet"
    return {"C√¢u h·ªèi": query, "Tr·∫£ l·ªùi": final_state.get('result', "L·ªói."), "Ph∆∞∆°ng ph√°p": final_decision_text}

if __name__ == "__main__":
    conversation_history = []
    while True:
        user_query = input("\n‚ùì Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (ho·∫∑c g√µ 'exit' ƒë·ªÉ tho√°t): ")
        if user_query.lower() in ['exit', 'quit']: break
        if not user_query.strip(): continue
        
        print("\nCh·ªçn ph∆∞∆°ng ph√°p bi·∫øn ƒë·ªïi truy v·∫•n...")
        choice = input("(Enter: regular, 1: rewrite, 2: decompose): ").strip()
        transform_map = {'1': 'rewrite', '2': 'decompose'}
        selected_transform = transform_map.get(choice)

        result = rag_pipeline(user_query, conversation_history, transformation_type=selected_transform)
        
        print("\n" + "="*20 + " K·∫æT QU·∫¢ " + "="*20)
        print(f"C√¢u tr·∫£ l·ªùi:\n{result['Tr·∫£ l·ªùi']}")
        print(f"(Ph∆∞∆°ng ph√°p: {result['Ph∆∞∆°ng ph√°p']})")
        print("=" * 50)
        conversation_history.append((user_query, result['Tr·∫£ l·ªùi']))