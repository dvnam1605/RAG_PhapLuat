import os
import google.generativeai as genai
from dotenv import load_dotenv
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS

load_dotenv()
API_KEY = os.getenv("API_KEY")
if not API_KEY:
    raise ValueError("API key for Google Generative AI is not set in the environment variables.")
MODEL = "gemini-1.5-flash"
embeddings = GPT4AllEmbeddings(model_file="model/all-MiniLM-L6-v2-f16.gguf")


genai.configure(api_key=API_KEY)
model = genai.GenerativeModel(MODEL)

path_db = "vector_store/faiss"
# T·∫£i l·∫°i vector store t·ª´ file ƒë√£ l∆∞u
vector_store = FAISS.load_local(
    path_db, 
    embeddings,
    allow_dangerous_deserialization=True  # Add this parameter
)

number_retrievals = 10 # S·ªë l∆∞·ª£ng vƒÉn b·∫£n cu·ªëi c√πng tr·∫£ v·ªÅ
mmr_retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={
        'k': number_retrievals, # S·ªë l∆∞·ª£ng vƒÉn b·∫£n cu·ªëi c√πng tr·∫£ v·ªÅ
        'fetch_k': 100, # S·ªë l∆∞·ª£ng vƒÉn b·∫£n ban ƒë·∫ßu c·∫ßn l·∫•y ƒë·ªÉ ch·ªçn l·ªçc, n√™n l·ªõn h∆°n k
        'lambda_mult': 0.3 # Gi√° tr·ªã t·ª´ 0 ƒë·∫øn 1. 0.5 l√† c√¢n b·∫±ng, 1 l√† diversity, 0 l√† similarity.
    }
)


class QueryTransformer:
    def __init__(self, model, query: str):
        self.model = model
        self.query = query

    def rewrite_query(self):
        """
        Rewrites a query to make it more specific and detailed for better retrieval using Gemini.
        
        Args:
            original_query (str): The original user query
        
        Returns:
            str: The rewritten query
        """
        # Prompt h∆∞·ªõng d·∫´n gi·ªëng nh∆∞ 'system' trong OpenAI
        system_prompt = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n gia trong vi·ªác c·∫£i thi·ªán truy v·∫•n t√¨m ki·∫øm th√¥ng tin trong lƒ©nh v·ª±c ph√°p lu·∫≠t. "
        "Nhi·ªám v·ª• c·ªßa b·∫°n l√† vi·∫øt l·∫°i truy v·∫•n ƒë·∫ßu v√†o c·ªßa ng∆∞·ªùi d√πng sao cho c·ª• th·ªÉ h∆°n, chi ti·∫øt h∆°n, r√µ r√†ng h∆°n v·ªÅ m·∫∑t ph√°p l√Ω. "
        "Truy v·∫•n m·ªõi n√™n b·ªï sung c√°c kh√°i ni·ªám ph√°p l√Ω, ƒëi·ªÅu kho·∫£n lu·∫≠t, m·ªëc th·ªùi gian, h√†nh vi vi ph·∫°m ho·∫∑c ch·ªß th·ªÉ li√™n quan, "
        "nh·∫±m tƒÉng kh·∫£ nƒÉng truy xu·∫•t ƒë√∫ng th√¥ng tin t·ª´ c∆° s·ªü tri th·ª©c ph√°p lu·∫≠t. "
        "M·ª•c ti√™u l√† t·∫°o ra m·ªôt truy v·∫•n s·∫Øc n√©t, r√µ r√†ng v√† ch·ª©a ƒë·ªß ng·ªØ c·∫£nh ƒë·ªÉ h·ªá th·ªëng hi·ªÉu ch√≠nh x√°c m·ª•c ƒë√≠ch t√¨m ki·∫øm."
    )


        
        # N·ªôi dung y√™u c·∫ßu
        user_prompt = f"""
    H√£y vi·∫øt l·∫°i truy v·∫•n ph√°p l√Ω d∆∞·ªõi ƒë√¢y sao cho c·ª• th·ªÉ v√† chi ti·∫øt h∆°n. 
    Truy v·∫•n m·ªõi c·∫ßn b·ªï sung:
    - C√°c thu·∫≠t ng·ªØ ph√°p l√Ω chuy√™n ng√†nh (n·∫øu c√≥).
    - C√°c y·∫øu t·ªë nh∆∞ h√†nh vi, ƒë·ªëi t∆∞·ª£ng, khung th·ªùi gian, lo·∫°i quan h·ªá ph√°p lu·∫≠t ho·∫∑c ƒëi·ªÅu kho·∫£n lu·∫≠t c√≥ th·ªÉ √°p d·ª•ng.
    - C√°c t√¨nh hu·ªëng t∆∞∆°ng ƒë∆∞∆°ng ho·∫∑c quy ƒë·ªãnh t∆∞∆°ng ·ª©ng trong lu·∫≠t hi·ªán h√†nh.

    M·ª•c ti√™u l√† ƒë·ªÉ truy v·∫•n ƒë∆∞·ª£c r√µ r√†ng, ch√≠nh x√°c h∆°n v√† d·ªÖ d√†ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p t·ª´ h·ªá th·ªëng ph√°p lu·∫≠t.

    üîç Truy v·∫•n g·ªëc: {self.query}

    ‚úèÔ∏è Truy v·∫•n ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i:
    """



        # G·ªôp system + user prompt th√†nh 1 ƒëo·∫°n duy nh·∫•t v√¨ Gemini kh√¥ng t√°ch "system"/"user"
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        # G·ªçi model sinh ra k·∫øt qu·∫£
        response = self.model.generate_content(
            full_prompt,
            generation_config=genai.types.GenerationConfig(
            temperature=0.0
            )
        )
        
        # Tr·∫£ k·∫øt qu·∫£ sau khi strip
        return response.text.strip()
    
    def generate_step_back_query(self):
        """
        Generates a more general 'step-back' query to retrieve broader context.
        
        Args:
            original_query (str): The original user query
            model (str): The model to use for step-back query generation
            
        Returns:
            str: The step-back query
        """
        # Define the system prompt to guide the AI assistant's behavior
        system_prompt = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n gia trong vi·ªác m·ªü r·ªông v√† c·∫£i thi·ªán c√°c truy v·∫•n t√¨m ki·∫øm th√¥ng tin ph√°p lu·∫≠t. "
        "Nhi·ªám v·ª• c·ªßa b·∫°n l√† nh·∫≠n truy v·∫•n ƒë·∫ßu v√†o t·ª´ ng∆∞·ªùi d√πng v√† t·∫°o ra m·ªôt phi√™n b·∫£n truy v·∫•n mang t√≠nh kh√°i qu√°t, m·ªü r·ªông h∆°n ‚Äî c√≤n g·ªçi l√† 'step-back query'. "
        "Truy v·∫•n m·ªü r·ªông c·∫ßn bao g·ªìm c√°c y·∫øu t·ªë ph√°p l√Ω li√™n quan nh∆∞: lo·∫°i tranh ch·∫•p, lu·∫≠t √°p d·ª•ng, b·ªëi c·∫£nh ph√°p l√Ω, th·ªùi ƒëi·ªÉm x·∫£y ra, c∆° quan c√≥ th·∫©m quy·ªÅn, v√† c√°c ƒëi·ªÅu kho·∫£n lu·∫≠t ti·ªÅm nƒÉng. "
        "Truy v·∫•n m·ªõi n√™n gi√∫p truy xu·∫•t ƒë∆∞·ª£c th√¥ng tin n·ªÅn, √°n l·ªá, nguy√™n t·∫Øc ph√°p lu·∫≠t ho·∫∑c quy ƒë·ªãnh c√≥ li√™n quan, nh·∫±m cung c·∫•p b·ªëi c·∫£nh ƒë·∫ßy ƒë·ªß h∆°n ƒë·ªÉ h·ªó tr·ª£ truy v·∫•n ban ƒë·∫ßu trong h·ªá th·ªëng RAG."
        )

        # Define the user prompt with the original query to be generalized
        user_prompt = f"""
    Truy v·∫•n sau ƒë√¢y ƒë∆∞·ª£c ƒë∆∞a ra b·ªüi ng∆∞·ªùi d√πng li√™n quan ƒë·∫øn m·ªôt v·∫•n ƒë·ªÅ ph√°p l√Ω c·ª• th·ªÉ. 
    Nhi·ªám v·ª• c·ªßa b·∫°n l√† m·ªü r·ªông truy v·∫•n n√†y th√†nh m·ªôt truy v·∫•n 'step-back' ‚Äî mang t√≠nh t·ªïng qu√°t v√† kh√°i qu√°t h∆°n, nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c ng·ªØ c·∫£nh ph√°p l√Ω c·ªët l√µi. 

    Truy v·∫•n m·ªü r·ªông c·∫ßn h∆∞·ªõng ƒë·∫øn vi·ªác:
    - Bao ph·ªß c√°c kh√≠a c·∫°nh ph√°p lu·∫≠t li√™n quan nh∆∞: ƒëi·ªÅu lu·∫≠t, ph·∫°m vi √°p d·ª•ng, kh√°i ni·ªám ph√°p l√Ω chung, ho·∫∑c c√°c lo·∫°i tranh ch·∫•p t∆∞∆°ng t·ª±.
    - L√†m r√µ b·ªëi c·∫£nh chung, c√°c khung ph√°p l√Ω, c∆° s·ªü ph√°p l√Ω c√≥ th·ªÉ √°p d·ª•ng cho truy v·∫•n g·ªëc.
    - T·∫°o ƒëi·ªÅu ki·ªán ƒë·ªÉ h·ªá th·ªëng c√≥ th·ªÉ truy xu·∫•t ƒë∆∞·ª£c th√™m th√¥ng tin h·ªó tr·ª£, l√†m r√µ ho·∫∑c cung c·∫•p n·ªÅn t·∫£ng gi·∫£i th√≠ch cho truy v·∫•n c·ª• th·ªÉ.

    **Truy v·∫•n g·ªëc (original query)**: {self.query}

    üîç **Truy v·∫•n m·ªü r·ªông (step-back query)**:
        """


        
        # Generate the step-back query using the specified model
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        # G·ªçi model sinh ra k·∫øt qu·∫£
        response = self.model.generate_content(
            full_prompt,
            generation_config=genai.types.GenerationConfig(
            temperature=0.1 
            )
        )
        
        # Tr·∫£ k·∫øt qu·∫£ sau khi strip
        return response.text.strip()
    
    def decompose_query(self, num_subqueries=4):
        """
        Decomposes a complex query into simpler sub-queries.
        
        Args:
            original_query (str): The original user query
        
        Returns:
            list: A list of decomposed sub-queries
        """
        # Define the system prompt to guide the AI assistant's behavior
        system_prompt = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n gia trong vi·ªác ph√¢n t√≠ch v√† ph√¢n r√£ truy v·∫•n t√¨m ki·∫øm trong lƒ©nh v·ª±c ph√°p lu·∫≠t. "
        "Nhi·ªám v·ª• c·ªßa b·∫°n l√† nh·∫≠n m·ªôt truy v·∫•n ph·ª©c t·∫°p t·ª´ ng∆∞·ªùi d√πng ‚Äî th∆∞·ªùng ch·ª©a nhi·ªÅu kh√≠a c·∫°nh ph√°p l√Ω kh√°c nhau ‚Äî "
        "v√† ph√¢n t√°ch truy v·∫•n n√†y th√†nh c√°c truy v·∫•n con ƒë∆°n gi·∫£n, m·ªói truy v·∫•n t·∫≠p trung v√†o m·ªôt y·∫øu t·ªë ho·∫∑c v·∫•n ƒë·ªÅ ph√°p l√Ω c·ª• th·ªÉ. "
        "ƒêi·ªÅu n√†y s·∫Ω gi√∫p h·ªá th·ªëng t√¨m ki·∫øm ho·∫°t ƒë·ªông hi·ªáu qu·∫£ h∆°n, truy xu·∫•t ƒë∆∞·ª£c th√¥ng tin ch√≠nh x√°c h∆°n, "
        "ƒë·∫∑c bi·ªát trong c√°c h·ªá th·ªëng RAG s·ª≠ d·ª•ng c∆° s·ªü tri th·ª©c lu·∫≠t ho·∫∑c vƒÉn b·∫£n ph√°p quy."
    )

        
        # Define the user prompt with the original query to be decomposed
        user_prompt = f"""
    Truy v·∫•n sau ƒë√¢y ch·ª©a nhi·ªÅu kh√≠a c·∫°nh ph√°p l√Ω kh√°c nhau, c·∫ßn ƒë∆∞·ª£c ph√¢n t√°ch ƒë·ªÉ h·ªá th·ªëng c√≥ th·ªÉ t√¨m ki·∫øm ch√≠nh x√°c h∆°n. 
    H√£y chia nh·ªè truy v·∫•n ph·ª©c t·∫°p n√†y th√†nh {num_subqueries} truy v·∫•n con, m·ªói truy v·∫•n n√™n t·∫≠p trung v√†o **m·ªôt kh√≠a c·∫°nh ri√™ng bi·ªát**, 
    ch·∫≥ng h·∫°n nh∆∞: h√†nh vi vi ph·∫°m, ch·ªß th·ªÉ li√™n quan, cƒÉn c·ª© ph√°p l√Ω, quy tr√¨nh x·ª≠ l√Ω, ho·∫∑c khung h√¨nh ph·∫°t √°p d·ª•ng.

    Y√™u c·∫ßu ƒë·ªãnh d·∫°ng:
    1. [Truy v·∫•n con 1]
    2. [Truy v·∫•n con 2]
    ...

    üîç Truy v·∫•n g·ªëc: {self.query}
    """


        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        response = self.model.generate_content(
            full_prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.2  # Gi·ªØ ƒë·ªô ng·∫´u nhi√™n th·∫•p ƒë·ªÉ truy v·∫•n con c√≥ t√≠nh ch√≠nh x√°c cao
            )
        )
        content = response.text.strip()
        # print(f"N·ªôi dung tr·∫£ v·ªÅ t·ª´ model: {content}")
        lines = content.split('\n')
        # print(f"C√≥ {len(lines)} d√≤ng trong n·ªôi dung tr·∫£ v·ªÅ t·ª´ model.")
        
        subqueries = []
        for line in lines:
            print(f"ƒêang x·ª≠ l√Ω d√≤ng: {line.strip()}")
            if line.strip() and any(line.strip().startswith(f"{i}.") for i in range(1, 10)):
                query = line.strip()
                query = query[query.find('.') + 1:].strip()  # Lo·∫°i b·ªè s·ªë th·ª© t·ª±
                subqueries.append(query)
        return subqueries  # Tr·∫£ v·ªÅ ƒë√∫ng s·ªë l∆∞·ª£ng truy v·∫•n con y√™u c·∫ßu

def transformed_search(query, transformation_type, top_k=3):
    """
    Search using a transformed query.
    
    Args:
        query (str): Original query
        vector_store (SimpleVectorStore): Vector store to search
        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')
        top_k (int): Number of results to return
        
    Returns:
        List[Dict]: Search results
    """
    print(f"Transformation type: {transformation_type}")
    print(f"Original query: {query}")
    
    results = []

    query_transformer = QueryTransformer(model, query)
    
    if transformation_type == "rewrite":
        # Query rewriting
        transformed_query = query_transformer.rewrite_query()
        print(f"Rewritten query: {transformed_query}")
        
        # FIX: Pass the transformed query string directly to the retriever.
        # The retriever will handle the embedding.
        results = mmr_retriever.invoke(transformed_query)
        
    elif transformation_type == "step_back":
        # Step-back prompting
        transformed_query = query_transformer.generate_step_back_query()
        print(f"Step-back query: {transformed_query}")
        
        results = mmr_retriever.invoke(transformed_query)
        
    elif transformation_type == "decompose":
        # Sub-query decomposition
        sub_queries = query_transformer.decompose_query(num_subqueries= top_k)
        print("Decomposed into sub-queries:")
        
        # Search with each sub-query and combine results
        all_results = []
        for sub_q in sub_queries:
            sub_results = mmr_retriever.invoke(sub_q)
            all_results.extend(sub_results)
        
        # 2. L·ªçc ra c√°c t√†i li·ªáu duy nh·∫•t (unique)
        unique_docs = []
        seen_contents = set()  # D√πng set ƒë·ªÉ ki·ªÉm tra tr√πng l·∫∑p hi·ªáu qu·∫£
        for doc in all_results:
            # Ch·ªâ th√™m t√†i li·ªáu v√†o k·∫øt qu·∫£ n·∫øu n·ªôi dung c·ªßa n√≥ ch∆∞a t·ª´ng xu·∫•t hi·ªán
            if doc.page_content not in seen_contents:
                unique_docs.append(doc)
                seen_contents.add(doc.page_content)
        
        # 3. L·∫•y top_k t·ª´ danh s√°ch c√°c t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c l·ªçc duy nh·∫•t
        results = unique_docs[:top_k]
        
    else:
        # FIX: For regular search, pass the original query string directly.
        results = mmr_retriever.invoke(query)
    
    return results